{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from gpytorch.kernels.kernel import Distance\n",
    "from timm.models.layers import PatchEmbed\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.,prune_amount = 0.7, seq_len = 197, type_ = 'gmm', prune_token_amount = None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.prune_token_amount = prune_token_amount\n",
    "        self.seq_len = seq_len\n",
    "        self.amount = seq_len - math.floor(prune_amount*seq_len)\n",
    "        self.prune_amount = prune_amount\n",
    "        self.type = type_\n",
    "        \n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "       \n",
    "        self.dist = Distance()\n",
    "        if self.prune_token_amount is None:\n",
    "            self.register_buffer('pi_mask', torch.ones(1, self.num_heads, seq_len, seq_len))\n",
    "        else:\n",
    "            self.register_buffer('pi_mask', torch.ones(1, self.num_heads, seq_len, int(seq_len*(1 - prune_token_amount))))\n",
    "\n",
    "    def forward(self, x, x_k = None):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, -1).permute(0,2,1,3)\n",
    "        if x_k is not None:\n",
    "            k = self.k(x_k).reshape(B, x_k.shape[1], self.num_heads, -1).permute(0,2,1,3)\n",
    "            v = self.v(x_k).reshape(B, x_k.shape[1], self.num_heads, -1).permute(0,2,1,3)\n",
    "        else:\n",
    "            k = self.k(x).reshape(B, N, self.num_heads, -1).permute(0,2,1,3)\n",
    "            v = self.v(x).reshape(B, N, self.num_heads, -1).permute(0,2,1,3)\n",
    "\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v = qkv[0], qkv[1], qkv[2]   \n",
    " \n",
    "        ### to calculate flops\n",
    "        if self.type == 'gmm':\n",
    "            if self.prune_token_amount is not None:\n",
    "                self.amount = self.seq_len - math.floor((self.prune_amount - self.prune_token_amount)*self.seq_len) \n",
    "            attn = (-self.scale/2.0)*self.dist._sq_dist(q[:,:, :self.amount, :], k, postprocess = False)\n",
    "            # attn = torch.einsum('bhle,bhme->bhlm',q[:,:, :self.amount, :]/self.scale,k)\n",
    "            attn =F.pad(attn, (0,0,self.seq_len- self.amount,0), 'constant', 0)\n",
    "            print(attn.shape, self.pi_mask.shape)\n",
    "            attn = self.pi_mask*torch.exp(attn)\n",
    "        else:\n",
    "            attn = torch.einsum('bhle,bhme->bhlm',q/self.scale,k)\n",
    "            attn = torch.exp(attn)\n",
    "        attn = attn / (attn.sum(dim=-1, keepdim = True) + 1e-6)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = PatchEmbed(img_size=224, patch_size=4, in_chans=3, embed_dim=192)\n",
    "num_patches = patch_embed.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [197, 785, 3137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::div encountered 2 time(s)\n",
      "Unsupported operator aten::exp encountered 1 time(s)\n",
      "Unsupported operator aten::sum encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::sub encountered 2 time(s)\n",
      "Unsupported operator aten::pow encountered 2 time(s)\n",
      "Unsupported operator aten::sum encountered 3 time(s)\n",
      "Unsupported operator aten::ones_like encountered 2 time(s)\n",
      "Unsupported operator aten::mul encountered 3 time(s)\n",
      "Unsupported operator aten::clamp_min_ encountered 1 time(s)\n",
      "Unsupported operator aten::exp encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "Unsupported operator aten::mean encountered 1 time(s)\n",
      "Unsupported operator aten::sub encountered 2 time(s)\n",
      "Unsupported operator aten::pow encountered 2 time(s)\n",
      "Unsupported operator aten::sum encountered 3 time(s)\n",
      "Unsupported operator aten::ones_like encountered 2 time(s)\n",
      "Unsupported operator aten::mul encountered 3 time(s)\n",
      "Unsupported operator aten::clamp_min_ encountered 1 time(s)\n",
      "Unsupported operator aten::exp encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 197, 197]) torch.Size([1, 4, 197, 197])\n",
      "torch.Size([1, 4, 197, 167]) torch.Size([1, 4, 197, 167])\n",
      "1.2165730134915935\n"
     ]
    }
   ],
   "source": [
    "seq_len = 197\n",
    "num_head = 4\n",
    "dim = 192\n",
    "x = torch.randn(1, seq_len, dim)\n",
    "prune_token_amount = 0.15\n",
    "\n",
    "model = GMMAttention(dim=dim, num_heads=4, qkv_bias= True, seq_len = seq_len, type_= 'softmax')\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "flop_count_softmax = flops.total()/1e9\n",
    "\n",
    "model = GMMAttention(dim=dim, num_heads=4, qkv_bias= True, seq_len = seq_len, type_= 'gmm')\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "flop_count_gmm = flops.total()/1e9\n",
    "\n",
    "model = GMMAttention(dim=dim, num_heads=4, qkv_bias= True, seq_len = seq_len, type_= 'gmm', prune_token_amount=0.15)\n",
    "x_k = x[:,:int(seq_len*(1 - prune_token_amount)),:]\n",
    "flops = FlopCountAnalysis(model, (x, x_k))\n",
    "flop_count_gmm_key = flops.total()/1e9\n",
    "\n",
    "print(flop_count_softmax/flop_count_gmm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1308660730091684"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_count_softmax/flop_count_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2165730134915935"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_count_softmax/flop_count_gmm_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27112016"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_count_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
